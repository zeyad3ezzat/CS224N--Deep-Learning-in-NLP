{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_assigment 3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1mFKOJ_lkfenhghl5VGYrV_9V5Qtl4ouO","authorship_tag":"ABX9TyNVu9Adz7T5KcYLa6ImYmvz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Rp6n1AqmAhR5","colab_type":"code","outputId":"50f28f11-41d8-45f8-b9bf-335d76b2036c","executionInfo":{"status":"ok","timestamp":1584406026465,"user_tz":-120,"elapsed":961,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["%cd \"/content/drive/My Drive/NLP Course/a3.zip (Unzipped Files)/student\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/NLP Course/a3.zip (Unzipped Files)/student\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8vIboB8HBa3M","colab_type":"text"},"source":["#parser transition"]},{"cell_type":"code","metadata":{"id":"Qako5jlTBNcC","colab_type":"code","colab":{}},"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","CS224N 2019-20: Homework 3\n","parser_transitions.py: Algorithms for completing partial parsess.\n","Sahil Chopra <schopra8@stanford.edu>\n","Haoshen Hong <haoshen@stanford.edu>\n","\"\"\"\n","\n","import sys\n","\n","class PartialParse(object):\n","    def __init__(self, sentence):\n","        \"\"\"Initializes this partial parse.\n","\n","        @param sentence (list of str): The sentence to be parsed as a list of words.\n","                                        Your code should not modify the sentence.\n","        \"\"\"\n","        # The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.\n","        self.sentence = sentence\n","\n","        ### YOUR CODE HERE (3 Lines)\n","        ### Your code should initialize the following fields:\n","        ###     self.stack: The current stack represented as a list with the top of the stack as the\n","        ###                 last element of the list.\n","        ###     self.buffer: The current buffer represented as a list with the first item on the\n","        ###                  buffer as the first item of the list\n","        ###     self.dependencies: The list of dependencies produced so far. Represented as a list of\n","        ###             tuples where each tuple is of the form (head, dependent).\n","        ###             Order for this list doesn't matter.\n","        ###\n","        ### Note: The root token should be represented with the string \"ROOT\"\n","        ###\n","        self.stack = ['ROOT']\n","        # self.stack.append(\"ROOT\")\n","        self.buffer=self.sentence.copy()\n","\n","      \n","        self.dependencies=[]\n","        ### END YOUR CODE\n","\n","\n","    def parse_step(self, transition):\n","        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n","\n","        @param transition (str): A string that equals \"S\", \"LA\", or \"RA\" representing the shift,\n","                                left-arc, and right-arc transitions. You can assume the provided\n","                                transition is a legal transition.\n","        \"\"\"\n","        ### YOUR CODE HERE (~7-10 Lines)\n","        ### TODO:\n","        ###     Implement a single parsing step, i.e. the logic for the following as\n","        ###     described in the pdf handout:\n","        ###         1. Shift\n","        ###         2. Left Arc\n","        ###         3. Right Arc\n","        \n","        if transition=='S' and len(self.buffer)>0:     #shift\n","          from_buffer_to_stack=self.buffer.pop(0)\n","          self.stack.append(from_buffer_to_stack)\n","      \n","        elif transition=='LA': #left arc\n","          second_word=self.stack.pop(-2)   #remove the second word from stack\n","          first_word=self.stack[-1]\n","          self.dependencies.append((first_word,second_word))\n","\n","        elif transition=='RA': #right arc\n","          first_word=self.stack.pop()   #remove the first word from stack\n","          second_word=self.stack[-1]\n","\n","          self.dependencies.append((second_word,first_word))\n","        # if transition == 'S':\n","        #     self.stack.append(self.buffer[0])\n","        #     del self.buffer[0]\n","        # elif transition == 'LA':\n","        #     assert len(self.stack) > 1\n","        #     self.dependencies.append((self.stack[-1],self.stack[-2]))\n","        #     del self.stack[-2]\n","        # elif transition == 'RA':\n","        #     assert len(self.stack) > 1\n","        #     self.dependencies.append((self.stack[-2],self.stack[-1]))\n","        #     del self.stack[-1]\n","\n","\n","          \n","\n","\n","        ### END YOUR CODE\n","\n","    def parse(self, transitions):\n","        \"\"\"Applies the provided transitions to this PartialParse\n","\n","        @param transitions (list of str): The list of transitions in the order they should be applied\n","\n","        @return dsependencies (list of string tuples): The list of dependencies produced when\n","                                                        parsing the sentence. Represented as a list of\n","                                                        tuples where each tuple is of the form (head, dependent).\n","        \"\"\"\n","\n","\n","\n","        for transition in transitions:\n","\n","            self.parse_step(transition)\n","        return self.dependencies\n","\n","\n","def minibatch_parse(sentences, model, batch_size):\n","    \"\"\"Parses a list of sentences in minibatches using a model.\n","\n","    @param sentences (list of list of str): A list of sentences to be parsed\n","                                            (each sentence is a list of words and each word is of type string)\n","    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function\n","                                model.predict(partial_parses) that takes in a list of PartialParses as input and\n","                                returns a list of transitions predicted for each parse. That is, after calling\n","                                    transitions = model.predict(partial_parses)\n","                                transitions[i] will be the next transition to apply to partial_parses[i].\n","    @param batch_size (int): The number of PartialParses to include in each minibatch\n","\n","\n","    @return dependencies (list of dependency lists): A list where each element is the dependencies\n","                                                    list for a parsed sentence. Ordering should be the\n","                                                    same as in sentences (i.e., dependencies[i] should\n","                                                    contain the parse for sentences[i]).\n","    \"\"\"\n","    dependencies = []\n","\n","    ### YOUR CODE HERE (~8-10 Lines)\n","    ### TODO:\n","    ###     Implement the minibatch parse algorithm as described in the pdf handout\n","    ###\n","    ###     Note: A shallow copy (as denoted in the PDF) can be made with the \"=\" sign in python, e.g.\n","    ###                 unfinished_parses = partial_parses[:].\n","    ###             Here `unfinished_parses` is a shallow copy of `partial_parses`.\n","    ###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances\n","    ###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.\n","    ###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`\n","    ###             contains references to the same objects. Thus, you should NOT use the `del` operator\n","    ###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that\n","    ###             is being accessed by `partial_parses` and may cause your code to crash.\n","    partial_parses=[]\n","    \n","      # print(sentence)\n","    \n","    # partial_parses    = [PartialParse(sentence) for sentence in sentences]\n","\n","      # print(partial_parses[0].stack[1])\n","\n","    # index=0\n","    # print(len(unfinished_parses))\n","    # no_batches=len(unfinished_parses)//batch_size\n","    # print(no_batches)\n","\n","    #from here \n","    for sentence in sentences:\n","      partial_parses.append(PartialParse(sentence))\n","    unfinished_parses = partial_parses[:]\n","\n","    while len(unfinished_parses) > 0:\n","        parsers = unfinished_parses[:batch_size]\n","        batch_transitions = model.predict(parsers)\n","        for pp, transition in zip(parsers, batch_transitions):\n","            pp.parse([transition])\n","            # print(pp.sentence)\n","\n","            if len(pp.buffer) == 0 and len(pp.stack) == 1:\n","                unfinished_parses.remove(pp)\n","\n","    dependencies = [pp.dependencies for pp in partial_parses]\n","    # to here\n","    # i=0\n","    # while len(unfinished_parses) > 0 and i<no_batches:\n","    #   minibatch=unfinished_parses[index:index+batch_size]\n","    #   i+=1\n","    #   print(minibatch)\n","    #   index=index+batch_size\n","\n","    #   transitions = model.predict(minibatch)\n","    #   for parser,transition in zip(minibatch,transitions):\n","        \n","    #     print(parser.sentence)\n","    #     parser.parse([transition])\n","    #     if len(parser.buffer) == 0 and len(parser.stack) == 1:\n","    #       unfinished_parses.remove(pp)\n","\n","    # while len(unfinished_parses) > 0:\n","    #   last_batch=unfinished_parses[index:]\n","    #   last_transitions = model.predict(minibatch)\n","    #   for parser,transitions in zip(last_batch,last_transitions):\n","    #     print(parser.sentence)\n","    #     parser.parse([transition])\n","    #     if len(parser.buffer) == 0 and len(parser.stack) == 1:\n","    #         unfinished_parses.remove(pp)\n","\n","\n","    # import math\n","    # for batch in range(int(math.ceil(len(sentences)/batch_size))):\n","    #     start_idx = batch*batch_size\n","    #     partialParses = [PartialParse(sentence) for sentence in sentences[start_idx:start_idx+batch_size]]\n","    #     unfin_partialParses = partialParses\n","    #     # 每个sentence，循环到停止条件为止\n","    #     while len(unfin_partialParses) != 0:\n","    #         transitions = model.predict(unfin_partialParses)\n","    #         for idx, transition in enumerate(transitions):\n","    #             unfin_partialParses[idx].parse([transition])\n","    #         unfin_partialParses = list(filter(lambda p: len(p.stack) != 1, unfin_partialParses))\n","\n","    #     for p in partialParses:\n","    #         dependencies.append(p.dependencies)\n","\n","\n","\n","\n","    ### END YOUR CODE\n","\n","    return dependencies\n","\n","\n","def test_step(name, transition, stack, buf, deps,\n","              ex_stack, ex_buf, ex_deps):\n","    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n","    pp = PartialParse([])\n","    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n","\n","    pp.parse_step(transition)\n","    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n","    assert stack == ex_stack, \\\n","        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n","    assert buf == ex_buf, \\\n","        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n","    assert deps == ex_deps, \\\n","        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n","    print(\"{:} test passed!\".format(name))\n","\n","\n","def test_parse_step():\n","    \"\"\"Simple tests for the PartialParse.parse_step function\n","    Warning: these are not exhaustive\n","    \"\"\"\n","    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n","              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n","    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n","              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n","    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n","              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n","\n","\n","def test_parse():\n","    \"\"\"Simple tests for the PartialParse.parse function\n","    Warning: these are not exhaustive\n","    \"\"\"\n","    sentence = [\"parse\", \"this\", \"sentence\"]\n","    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n","    dependencies = tuple(sorted(dependencies))\n","    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n","    assert dependencies == expected,  \\\n","        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n","    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n","        \"parse test failed: the input sentence should not be modified\"\n","    print(\"parse test passed!\")\n","\n","\n","class DummyModel(object):\n","    \"\"\"Dummy model for testing the minibatch_parse function\n","    \"\"\"\n","    def __init__(self, mode = \"unidirectional\"):\n","        self.mode = mode\n","\n","    def predict(self, partial_parses):\n","        if self.mode == \"unidirectional\":\n","            return self.unidirectional_predict(partial_parses)\n","        elif self.mode == \"interleave\":\n","            return self.interleave_predict(partial_parses)\n","        else:\n","            raise NotImplementedError()\n","\n","    def unidirectional_predict(self, partial_parses):\n","        \"\"\"First shifts everything onto the stack and then does exclusively right arcs if the first word of\n","        the sentence is \"right\", \"left\" if otherwise.\n","        \"\"\"\n","        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n","                for pp in partial_parses]\n","\n","    def interleave_predict(self, partial_parses):\n","        \"\"\"First shifts everything onto the stack and then interleaves \"right\" and \"left\".\n","        \"\"\"\n","        return [(\"RA\" if len(pp.stack) % 2 == 0 else \"LA\") if len(pp.buffer) == 0 else \"S\"\n","                for pp in partial_parses]\n","\n","def test_dependencies(name, deps, ex_deps):\n","    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n","    deps = tuple(sorted(deps))\n","    assert deps == ex_deps, \\\n","        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n","\n","\n","def test_minibatch_parse():\n","    \"\"\"Simple tests for the minibatch_parse function\n","    Warning: these are not exhaustive\n","    \"\"\"\n","\n","    # Unidirectional arcs test\n","    sentences = [[\"right\", \"arcs\", \"only\"],\n","                 [\"right\", \"arcs\", \"only\", \"again\"],\n","                 [\"left\", \"arcs\", \"only\"],\n","                 [\"left\", \"arcs\", \"only\", \"again\"]]\n","    deps = minibatch_parse(sentences, DummyModel(), 2)\n","    test_dependencies(\"minibatch_parse\", deps[0],\n","                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n","    test_dependencies(\"minibatch_parse\", deps[1],\n","                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n","    test_dependencies(\"minibatch_parse\", deps[2],\n","                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n","    test_dependencies(\"minibatch_parse\", deps[3],\n","                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n","\n","    # Out-of-bound test\n","    sentences = [[\"right\"]]\n","    deps = minibatch_parse(sentences, DummyModel(), 2)\n","    test_dependencies(\"minibatch_parse\", deps[0], (('ROOT', 'right'),))\n","\n","    # Mixed arcs test\n","    sentences = [[\"this\", \"is\", \"interleaving\", \"dependency\", \"test\"]]\n","    deps = minibatch_parse(sentences, DummyModel(mode=\"interleave\"), 1)\n","    test_dependencies(\"minibatch_parse\", deps[0],\n","                      (('ROOT', 'is'), ('dependency', 'interleaving'),\n","                      ('dependency', 'test'), ('is', 'dependency'), ('is', 'this')))\n","    print(\"minibatch_parse test passed!\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4myaNroHHzmR","colab_type":"code","outputId":"bf322d50-8d3f-4dbc-9366-a670e002157e","executionInfo":{"status":"ok","timestamp":1584406026965,"user_tz":-120,"elapsed":915,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["test_parse_step()\n","test_parse()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["SHIFT test passed!\n","LEFT-ARC test passed!\n","RIGHT-ARC test passed!\n","parse test passed!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DM2X12qluTKE","colab_type":"code","outputId":"6aa4972f-27c1-4e65-c577-0289f1b837c4","executionInfo":{"status":"ok","timestamp":1584406026967,"user_tz":-120,"elapsed":765,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["test_minibatch_parse()\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["minibatch_parse test passed!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4JHEJnYvC5h-","colab_type":"text"},"source":["#neural network"]},{"cell_type":"code","metadata":{"id":"P6EnpFWEBedb","colab_type":"code","colab":{}},"source":["\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","CS224N 2019-20: Homework 3\n","parser_model.py: Feed-Forward Neural Network for Dependency Parsing\n","Sahil Chopra <schopra8@stanford.edu>\n","Haoshen Hong <haoshen@stanford.edu>\n","\"\"\"\n","import argparse\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ParserModel(nn.Module):\n","    \"\"\" Feedforward neural network with an embedding layer and two hidden layers.\n","    The ParserModel will predict which transition should be applied to a\n","    given partial parse configuration.\n","\n","    PyTorch Notes:\n","        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n","            are a subclass of this \"nn.Module\".\n","        - The \"__init__\" method is where you define all the layers and parameters\n","            (embedding layers, linear layers, dropout layers, etc.).\n","        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n","            when you write \"m = ParserModel()\".\n","        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n","            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n","            in other ParserModel methods.\n","        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n","    \"\"\"\n","    def __init__(self, embeddings, n_features=36,\n","        hidden_size=200, n_classes=3, dropout_prob=0.5):\n","        \"\"\" Initialize the parser model.\n","\n","        @param embeddings (ndarray): word embeddings (num_words, embedding_size)\n","        @param n_features (int): number of input features\n","        @param hidden_size (int): number of hidden units\n","        @param n_classes (int): number of output classes\n","        @param dropout_prob (float): dropout probability\n","        \"\"\"\n","        super(ParserModel, self).__init__()\n","        self.n_features = n_features\n","        self.n_classes = n_classes\n","        self.dropout_prob = dropout_prob\n","        self.embed_size = embeddings.shape[1]\n","        self.hidden_size = hidden_size\n","        self.embeddings = nn.Parameter(torch.tensor(embeddings))\n","        # self.embed_to_hidden_weight=nn.Parameter(n_features, self.hidden_size, bias=True)\n","        self.embed_to_hidden_weight = nn.Parameter(torch.Tensor(self.embed_size*self.n_features, self.hidden_size))\n","        nn.init.xavier_uniform_(self.embed_to_hidden_weight)\n","        self.embed_to_hidden_bias = nn.Parameter(torch.Tensor(1,self.hidden_size))\n","        nn.init.uniform_(self.embed_to_hidden_bias )  \n","\n","        self.dropout=nn.Dropout(self.dropout_prob)\n","\n","        self.hidden_to_logits_weight = nn.Parameter(torch.Tensor(self.hidden_size, self.n_classes))\n","        nn.init.xavier_uniform_(self.hidden_to_logits_weight)\n","        self.hidden_to_logits_bias = nn.Parameter(torch.Tensor(1,self.n_classes))\n","        nn.init.uniform_(self.hidden_to_logits_bias )  \n","\n","\n","\n","        ### YOUR CODE HERE (~10 Lines)\n","        ### TODO:\n","        ###     1) Declare `self.embed_to_hidden_weight` and `self.embed_to_hidden_bias` as `nn.Parameter`.\n","        ###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`\n","        ###        with default parameters.\n","        ###     2) Construct `self.dropout` layer.\n","        ###     3) Declare `self.hidden_to_logits_weight` and `self.hidden_to_logits_bias` as `nn.Parameter`.\n","        ###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`\n","        ###        with default parameters.\n","        ###\n","        ### Note: Trainable variables are declared as `nn.Parameter` which is a commonly used API\n","        ###       to include a tensor into a computational graph to support updating w.r.t its gradient.\n","        ###       Here, we use Xavier Uniform Initialization for our Weight initialization.\n","        ###       It has been shown empirically, that this provides better initial weights\n","        ###       for training networks than random uniform initialization.\n","        ###       For more details checkout this great blogpost:\n","        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n","        ###\n","        ### Please see the following docs for support:\n","        ###     nn.Parameter: https://pytorch.org/docs/stable/nn.html#parameters\n","        ###     Initialization: https://pytorch.org/docs/stable/nn.init.html\n","        ###     Dropout: https://pytorch.org/docs/stable/nn.html#dropout-layers\n","\n","\n","\n","\n","        ### END YOUR CODE\n","\n","    def embedding_lookup(self, w):\n","        \"\"\" Utilize `w` to select embeddings from embedding matrix `self.embeddings`\n","            @param w (Tensor): input tensor of word indices (batch_size, n_features)\n","\n","            @return x (Tensor): tensor of embeddings for words represented in w\n","                                (batch_size, n_features * embed_size)\n","        \"\"\"\n","\n","        ### YOUR CODE HERE (~1-3 Lines)\n","        ### TODO:\n","        ###     1) For each index `i` in `w`, select `i`th vector from self.embeddings\n","        ###     2) Reshape the tensor using `view` function if necessary\n","        ###\n","        ### Note: All embedding vectors are stacked and stored as a matrix. The model receives\n","        ###       a list of indices representing a sequence of words, then it calls this lookup\n","        ###       function to map indices to sequence of embeddings.\n","        ###\n","        ###       This problem aims to test your understanding of embedding lookup,\n","        ###       so DO NOT use any high level API like nn.Embedding\n","        ###       (we are asking you to implement that!). Pay attention to tensor shapes\n","        ###       and reshape if necessary. Make sure you know each tensor's shape before you run the code!\n","        ###\n","        ### Pytorch has some useful APIs for you, and you can use either one\n","        ### in this problem (except nn.Embedding). These docs might be helpful:\n","        ###     Index select: https://pytorch.org/docs/stable/torch.html#torch.index_select\n","        ###     Gather: https://pytorch.org/docs/stable/torch.html#torch.gather\n","        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n","        # x=torch.zeros(size=(w.shape[0],w.shape[1]*self.embeddings.shape[1]))\n","        # for i,vector in enumerate(w):\n","          # x[i]=torch.index_select(self.embeddings, 0, vector, out=None).reshape(-1)\n","        x = self.embeddings[w].view((w.shape[0], -1))\n","\n","        ### END YOUR CODE\n","        return x\n","\n","\n","    def forward(self, w):\n","        \"\"\" Run the model forward.\n","\n","            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n","\n","            PyTorch Notes:\n","                - Every nn.Module object (PyTorch model) has a `forward` function.\n","                - When you apply your nn.Module to an input tensor `w` this function is applied to the tensor.\n","                    For example, if you created an instance of your ParserModel and applied it to some `w` as follows,\n","                    the `forward` function would called on `w` and the result would be stored in the `output` variable:\n","                        model = ParserModel()\n","                        output = model(w) # this calls the forward function\n","                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n","\n","        @param w (Tensor): input tensor of tokens (batch_size, n_features)\n","\n","        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n","                                 without applying softmax (batch_size, n_classes)\n","        \"\"\"\n","        ### YOUR CODE HERE (~3-5 lines)\n","        ### TODO:\n","        ###     Complete the forward computation as described in write-up. In addition, include a dropout layer\n","        ###     as decleared in `__init__` after ReLU function.\n","        ###\n","        ### Note: We do not apply the softmax to the logits here, because\n","        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n","        ###\n","        ### Please see the following docs for support:\n","        ###     Matrix product: https://pytorch.org/docs/stable/torch.html#torch.matmul\n","        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n","        # print(w.shape)\n","        x=self.embedding_lookup(w)\n","        # print(x.shape)\n","        # print(self.embed_to_hidden_weight.shape)\n","        h=F.relu(torch.matmul(x,self.embed_to_hidden_weight)+self.embed_to_hidden_bias)\n","        logits=torch.matmul(self.dropout(h),self.hidden_to_logits_weight)+self.hidden_to_logits_bias\n","        \n","        ### END YOUR CODE\n","        return logits\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hG4tdKnuqHOU","colab_type":"code","colab":{}},"source":["\n","embeddings = np.zeros((100, 30), dtype=np.float32)\n","model = ParserModel(embeddings)\n","def check_forward():\n","        inputs =torch.randint(0, 100, (4, 36), dtype=torch.long)\n","        out = model(inputs)\n","        expected_out_shape = (4, 3)\n","        assert out.shape == expected_out_shape, \"The result shape of forward is: \" + repr(out.shape) + \\\n","                                                \" which doesn't match expected \" + repr(expected_out_shape)\n","check_forward()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7CL9fDtngP9z","colab_type":"code","colab":{}},"source":["\n","embeddings = np.zeros((100, 30), dtype=np.float32)\n","model = ParserModel(embeddings)\n","def check_embedding():\n","        inds = torch.randint(0, 100, (4, 36), dtype=torch.long)\n","        selected = model.embedding_lookup(inds)\n","        assert np.all(selected.data.numpy() == 0), \"The result of embedding lookup: \" \\\n","                                      + repr(selected) + \" contains non-zero elements.\"\n","\n","\n","check_embedding()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XIlyDPG-uNd6","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"F1gocCjS9DnG","colab_type":"text"},"source":["#general utils//parser utils\n","just copied it to start training"]},{"cell_type":"code","metadata":{"id":"mnnKIpge9DG0","colab_type":"code","colab":{}},"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","CS224N 2018-19: Homework 3\n","general_utils.py: General purpose utilities.\n","Sahil Chopra <schopra8@stanford.edu>\n","\"\"\"\n","\n","import sys\n","import time\n","import numpy as np\n","\n","\n","def get_minibatches(data, minibatch_size, shuffle=True):\n","    \"\"\"\n","    Iterates through the provided data one minibatch at at time. You can use this function to\n","    iterate through data in minibatches as follows:\n","\n","        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n","            ...\n","\n","    Or with multiple data sources:\n","\n","        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n","            ...\n","\n","    Args:\n","        data: there are two possible values:\n","            - a list or numpy array\n","            - a list where each element is either a list or numpy array\n","        minibatch_size: the maximum number of items in a minibatch\n","        shuffle: whether to randomize the order of returned data\n","    Returns:\n","        minibatches: the return value depends on data:\n","            - If data is a list/array it yields the next minibatch of data.\n","            - If data a list of lists/arrays it returns the next minibatch of each element in the\n","              list. This can be used to iterate through multiple data sources\n","              (e.g., features and labels) at the same time.\n","\n","    \"\"\"\n","    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n","    data_size = len(data[0]) if list_data else len(data)\n","    indices = np.arange(data_size)\n","    if shuffle:\n","        np.random.shuffle(indices)\n","    for minibatch_start in np.arange(0, data_size, minibatch_size):\n","        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n","        yield [_minibatch(d, minibatch_indices) for d in data] if list_data \\\n","            else _minibatch(data, minibatch_indices)\n","\n","\n","def _minibatch(data, minibatch_idx):\n","    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n","\n","\n","def test_all_close(name, actual, expected):\n","    if actual.shape != expected.shape:\n","        raise ValueError(\"{:} failed, expected output to have shape {:} but has shape {:}\"\n","                         .format(name, expected.shape, actual.shape))\n","    if np.amax(np.fabs(actual - expected)) > 1e-6:\n","        raise ValueError(\"{:} failed, expected {:} but value is {:}\".format(name, expected, actual))\n","    else:\n","        print(name, \"passed!\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vt3RFYo98xa8","colab_type":"code","colab":{}},"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","CS224N 2018-19: Homework 3\n","parser_utils.py: Utilities for training the dependency parser.\n","Sahil Chopra <schopra8@stanford.edu>\n","\"\"\"\n","\n","import time\n","import os\n","import logging\n","from collections import Counter\n","# from . general_utils import get_minibatches\n","# from parser_transitions import minibatch_parse\n","\n","from tqdm import tqdm\n","import torch\n","import numpy as np\n","\n","P_PREFIX = '<p>:'\n","L_PREFIX = '<l>:'\n","UNK = '<UNK>'\n","NULL = '<NULL>'\n","ROOT = '<ROOT>'\n","\n","\n","class Config(object):\n","    language = 'english'\n","    with_punct = True\n","    unlabeled = True\n","    lowercase = True\n","    use_pos = True\n","    use_dep = True\n","    use_dep = use_dep and (not unlabeled)\n","    data_path = './data'\n","    train_file = 'train.conll'\n","    dev_file = 'dev.conll'\n","    test_file = 'test.conll'\n","    embedding_file = './data/en-cw.txt'\n","\n","\n","class Parser(object):\n","    \"\"\"Contains everything needed for transition-based dependency parsing except for the model\"\"\"\n","\n","    def __init__(self, dataset):\n","        root_labels = list([l for ex in dataset\n","                           for (h, l) in zip(ex['head'], ex['label']) if h == 0])\n","        counter = Counter(root_labels)\n","        if len(counter) > 1:\n","            logging.info('Warning: more than one root label')\n","            logging.info(counter)\n","        self.root_label = counter.most_common()[0][0]\n","        deprel = [self.root_label] + list(set([w for ex in dataset\n","                                               for w in ex['label']\n","                                               if w != self.root_label]))\n","        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)}\n","        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n","\n","        config = Config()\n","        self.unlabeled = config.unlabeled\n","        self.with_punct = config.with_punct\n","        self.use_pos = config.use_pos\n","        self.use_dep = config.use_dep\n","        self.language = config.language\n","\n","        if self.unlabeled:\n","            trans = ['L', 'R', 'S']\n","            self.n_deprel = 1\n","        else:\n","            trans = ['L-' + l for l in deprel] + ['R-' + l for l in deprel] + ['S']\n","            self.n_deprel = len(deprel)\n","\n","        self.n_trans = len(trans)\n","        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n","        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n","\n","        # logging.info('Build dictionary for part-of-speech tags.')\n","        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n","                                  offset=len(tok2id)))\n","        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n","        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n","        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n","\n","        # logging.info('Build dictionary for words.')\n","        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n","                                  offset=len(tok2id)))\n","        tok2id[UNK] = self.UNK = len(tok2id)\n","        tok2id[NULL] = self.NULL = len(tok2id)\n","        tok2id[ROOT] = self.ROOT = len(tok2id)\n","\n","        self.tok2id = tok2id\n","        self.id2tok = {v: k for (k, v) in tok2id.items()}\n","\n","        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0)\n","        self.n_tokens = len(tok2id)\n","\n","    def vectorize(self, examples):\n","        vec_examples = []\n","        for ex in examples:\n","            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n","                                  else self.UNK for w in ex['word']]\n","            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n","                                   else self.P_UNK for w in ex['pos']]\n","            head = [-1] + ex['head']\n","            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n","                            else -1 for w in ex['label']]\n","            vec_examples.append({'word': word, 'pos': pos,\n","                                 'head': head, 'label': label})\n","        return vec_examples\n","\n","    def extract_features(self, stack, buf, arcs, ex):\n","        if stack[0] == \"ROOT\":\n","            stack[0] = 0\n","\n","        def get_lc(k):\n","            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n","\n","        def get_rc(k):\n","            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n","                          reverse=True)\n","\n","        p_features = []\n","        l_features = []\n","        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n","        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n","        if self.use_pos:\n","            p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n","            p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n","\n","        for i in range(2):\n","            if i < len(stack):\n","                k = stack[-i-1]\n","                lc = get_lc(k)\n","                rc = get_rc(k)\n","                llc = get_lc(lc[0]) if len(lc) > 0 else []\n","                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n","\n","                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n","                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n","                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n","                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n","                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n","                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n","\n","                if self.use_pos:\n","                    p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n","                    p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n","                    p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n","                    p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n","                    p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n","                    p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n","\n","                if self.use_dep:\n","                    l_features.append(ex['label'][lc[0]] if len(lc) > 0 else self.L_NULL)\n","                    l_features.append(ex['label'][rc[0]] if len(rc) > 0 else self.L_NULL)\n","                    l_features.append(ex['label'][lc[1]] if len(lc) > 1 else self.L_NULL)\n","                    l_features.append(ex['label'][rc[1]] if len(rc) > 1 else self.L_NULL)\n","                    l_features.append(ex['label'][llc[0]] if len(llc) > 0 else self.L_NULL)\n","                    l_features.append(ex['label'][rrc[0]] if len(rrc) > 0 else self.L_NULL)\n","            else:\n","                features += [self.NULL] * 6\n","                if self.use_pos:\n","                    p_features += [self.P_NULL] * 6\n","                if self.use_dep:\n","                    l_features += [self.L_NULL] * 6\n","\n","        features += p_features + l_features\n","        assert len(features) == self.n_features\n","        return features\n","\n","    def get_oracle(self, stack, buf, ex):\n","        if len(stack) < 2:\n","            return self.n_trans - 1\n","\n","        i0 = stack[-1]\n","        i1 = stack[-2]\n","        h0 = ex['head'][i0]\n","        h1 = ex['head'][i1]\n","        l0 = ex['label'][i0]\n","        l1 = ex['label'][i1]\n","\n","        if self.unlabeled:\n","            if (i1 > 0) and (h1 == i0):\n","                return 0\n","            elif (i1 >= 0) and (h0 == i1) and \\\n","                 (not any([x for x in buf if ex['head'][x] == i0])):\n","                return 1\n","            else:\n","                return None if len(buf) == 0 else 2\n","        else:\n","            if (i1 > 0) and (h1 == i0):\n","                return l1 if (l1 >= 0) and (l1 < self.n_deprel) else None\n","            elif (i1 >= 0) and (h0 == i1) and \\\n","                 (not any([x for x in buf if ex['head'][x] == i0])):\n","                return l0 + self.n_deprel if (l0 >= 0) and (l0 < self.n_deprel) else None\n","            else:\n","                return None if len(buf) == 0 else self.n_trans - 1\n","\n","    def create_instances(self, examples):\n","        all_instances = []\n","        succ = 0\n","        for id, ex in enumerate(examples):\n","            n_words = len(ex['word']) - 1\n","\n","            # arcs = {(h, t, label)}\n","            stack = [0]\n","            buf = [i + 1 for i in range(n_words)]\n","            arcs = []\n","            instances = []\n","            for i in range(n_words * 2):\n","                gold_t = self.get_oracle(stack, buf, ex)\n","                if gold_t is None:\n","                    break\n","                legal_labels = self.legal_labels(stack, buf)\n","                assert legal_labels[gold_t] == 1\n","                instances.append((self.extract_features(stack, buf, arcs, ex),\n","                                  legal_labels, gold_t))\n","                if gold_t == self.n_trans - 1:\n","                    stack.append(buf[0])\n","                    buf = buf[1:]\n","                elif gold_t < self.n_deprel:\n","                    arcs.append((stack[-1], stack[-2], gold_t))\n","                    stack = stack[:-2] + [stack[-1]]\n","                else:\n","                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n","                    stack = stack[:-1]\n","            else:\n","                succ += 1\n","                all_instances += instances\n","\n","        return all_instances\n","\n","    def legal_labels(self, stack, buf):\n","        labels = ([1] if len(stack) > 2 else [0]) * self.n_deprel\n","        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel\n","        labels += [1] if len(buf) > 0 else [0]\n","        return labels\n","\n","    def parse(self, dataset, eval_batch_size=5000):\n","        sentences = []\n","        sentence_id_to_idx = {}\n","        for i, example in enumerate(dataset):\n","            n_words = len(example['word']) - 1\n","            sentence = [j + 1 for j in range(n_words)]\n","            sentences.append(sentence)\n","            sentence_id_to_idx[id(sentence)] = i\n","\n","        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n","        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n","\n","        UAS = all_tokens = 0.0\n","        with tqdm(total=len(dataset)) as prog:\n","            for i, ex in enumerate(dataset):\n","                head = [-1] * len(ex['word'])\n","                for h, t, in dependencies[i]:\n","                    head[t] = h\n","                for pred_h, gold_h, gold_l, pos in \\\n","                        zip(head[1:], ex['head'][1:], ex['label'][1:], ex['pos'][1:]):\n","                        assert self.id2tok[pos].startswith(P_PREFIX)\n","                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n","                        if (self.with_punct) or (not punct(self.language, pos_str)):\n","                            UAS += 1 if pred_h == gold_h else 0\n","                            all_tokens += 1\n","                prog.update(i + 1)\n","        UAS /= all_tokens\n","        return UAS, dependencies\n","\n","\n","class ModelWrapper(object):\n","    def __init__(self, parser, dataset, sentence_id_to_idx):\n","        self.parser = parser\n","        self.dataset = dataset\n","        self.sentence_id_to_idx = sentence_id_to_idx\n","\n","    def predict(self, partial_parses):\n","        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dependencies,\n","                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n","                for p in partial_parses]\n","        mb_x = np.array(mb_x).astype('int32')\n","        mb_x = torch.from_numpy(mb_x).long()\n","        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n","\n","        pred = self.parser.model(mb_x)\n","        pred = pred.detach().numpy()\n","        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n","        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n","        return pred\n","\n","\n","def read_conll(in_file, lowercase=False, max_example=None):\n","    examples = []\n","    with open(in_file) as f:\n","        word, pos, head, label = [], [], [], []\n","        for line in f.readlines():\n","            sp = line.strip().split('\\t')\n","            if len(sp) == 10:\n","                if '-' not in sp[0]:\n","                    word.append(sp[1].lower() if lowercase else sp[1])\n","                    pos.append(sp[4])\n","                    head.append(int(sp[6]))\n","                    label.append(sp[7])\n","            elif len(word) > 0:\n","                examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n","                word, pos, head, label = [], [], [], []\n","                if (max_example is not None) and (len(examples) == max_example):\n","                    break\n","        if len(word) > 0:\n","            examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n","    return examples\n","\n","\n","def build_dict(keys, n_max=None, offset=0):\n","    count = Counter()\n","    for key in keys:\n","        count[key] += 1\n","    ls = count.most_common() if n_max is None \\\n","        else count.most_common(n_max)\n","\n","    return {w[0]: index + offset for (index, w) in enumerate(ls)}\n","\n","\n","def punct(language, pos):\n","    if language == 'english':\n","        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n","    elif language == 'chinese':\n","        return pos == 'PU'\n","    elif language == 'french':\n","        return pos == 'PUNC'\n","    elif language == 'german':\n","        return pos in [\"$.\", \"$,\", \"$[\"]\n","    elif language == 'spanish':\n","        # http://nlp.stanford.edu/software/spanish-faq.shtml\n","        return pos in [\"f0\", \"faa\", \"fat\", \"fc\", \"fd\", \"fe\", \"fg\", \"fh\",\n","                       \"fia\", \"fit\", \"fp\", \"fpa\", \"fpt\", \"fs\", \"ft\",\n","                       \"fx\", \"fz\"]\n","    elif language == 'universal':\n","        return pos == 'PUNCT'\n","    else:\n","        raise ValueError('language: %s is not supported.' % language)\n","\n","\n","def minibatches(data, batch_size):\n","    x = np.array([d[0] for d in data])\n","    y = np.array([d[2] for d in data])\n","    one_hot = np.zeros((y.size, 3))\n","    one_hot[np.arange(y.size), y] = 1\n","    return get_minibatches([x, one_hot], batch_size)\n","\n","\n","def load_and_preprocess_data(reduced=True):\n","    config = Config()\n","\n","    print(\"Loading data...\",)\n","    start = time.time()\n","    train_set = read_conll(os.path.join(config.data_path, config.train_file),\n","                           lowercase=config.lowercase)\n","    dev_set = read_conll(os.path.join(config.data_path, config.dev_file),\n","                         lowercase=config.lowercase)\n","    test_set = read_conll(os.path.join(config.data_path, config.test_file),\n","                          lowercase=config.lowercase)\n","    if reduced:\n","        train_set = train_set[:1000]\n","        dev_set = dev_set[:500]\n","        test_set = test_set[:500]\n","    print(\"took {:.2f} seconds\".format(time.time() - start))\n","\n","    print(\"Building parser...\",)\n","    start = time.time()\n","    parser = Parser(train_set)\n","    print(\"took {:.2f} seconds\".format(time.time() - start))\n","\n","    print(\"Loading pretrained embeddings...\",)\n","    start = time.time()\n","    word_vectors = {}\n","    for line in open(config.embedding_file).readlines():\n","        sp = line.strip().split()\n","        word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n","    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n","\n","    for token in parser.tok2id:\n","        i = parser.tok2id[token]\n","        if token in word_vectors:\n","            embeddings_matrix[i] = word_vectors[token]\n","        elif token.lower() in word_vectors:\n","            embeddings_matrix[i] = word_vectors[token.lower()]\n","    print(\"took {:.2f} seconds\".format(time.time() - start))\n","\n","    print(\"Vectorizing data...\",)\n","    start = time.time()\n","    train_set = parser.vectorize(train_set)\n","    dev_set = parser.vectorize(dev_set)\n","    test_set = parser.vectorize(test_set)\n","    print(\"took {:.2f} seconds\".format(time.time() - start))\n","\n","    print(\"Preprocessing training data...\",)\n","    start = time.time()\n","    train_examples = parser.create_instances(train_set)\n","    print(\"took {:.2f} seconds\".format(time.time() - start))\n","\n","    return parser, embeddings_matrix, train_examples, dev_set, test_set,\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","if __name__ == '__main__':\n","    pass\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKxRwiuOuRNU","colab_type":"text"},"source":["#training"]},{"cell_type":"code","metadata":{"id":"Tob011pvuSyq","colab_type":"code","colab":{}},"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","CS224N 2019-20: Homework 3\n","run.py: Run the dependency parser.\n","Sahil Chopra <schopra8@stanford.edu>\n","Haoshen Hong <haoshen@stanford.edu>\n","\"\"\"\n","from datetime import datetime\n","import os\n","import pickle\n","import math\n","import time\n","import argparse\n","\n","from torch import nn, optim\n","import torch\n","from tqdm import tqdm\n","\n","# from parser_model import ParserModel\n","# from utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter\n","\n","# parser = argparse.ArgumentParser(description='Train neural dependency parser in pytorch')\n","# parser.add_argument('-d', '--debug', action='store_true', help='whether to enter debug mode')\n","# args = parser.parse_args()\n","\n","# -----------------\n","# Primary Functions\n","# -----------------\n","def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n","    \"\"\" Train the neural dependency parser.\n","\n","    @param parser (Parser): Neural Dependency Parser\n","    @param train_data ():\n","    @param dev_data ():\n","    @param output_path (str): Path to which model weights and results are written.\n","    @param batch_size (int): Number of examples in a single batch\n","    @param n_epochs (int): Number of training epochs\n","    @param lr (float): Learning rate\n","    \"\"\"\n","    best_dev_UAS = 0\n","\n","\n","    ### YOUR CODE HERE (~2-7 lines)\n","    ### TODO:\n","    ###      1) Construct Adam Optimizer in variable `optimizer`\n","    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func` with `mean`\n","    ###         reduction (default)\n","    ###\n","    ### Hint: Use `parser.model.parameters()` to pass optimizer\n","    ###       necessary parameters to tune.\n","    ### Please see the following docs for support:\n","    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n","    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n","\n","    optimizer=optim.Adam(params=parser.model.parameters(),lr=lr)\n","    loss_func=nn.CrossEntropyLoss()\n","\n","    ### END YOUR CODE\n","\n","    for epoch in range(n_epochs):\n","        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n","        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n","        if dev_UAS > best_dev_UAS:\n","            best_dev_UAS = dev_UAS\n","            print(\"New best dev UAS! Saving model.\")\n","            torch.save(parser.model.state_dict(), output_path)\n","        print(\"\")\n","\n","\n","def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n","    \"\"\" Train the neural dependency parser for single epoch.\n","\n","    Note: In PyTorch we can signify train versus test and automatically have\n","    the Dropout Layer applied and removed, accordingly, by specifying\n","    whether we are training, `model.train()`, or evaluating, `model.eval()`\n","\n","    @param parser (Parser): Neural Dependency Parser\n","    @param train_data ():\n","    @param dev_data ():\n","    @param optimizer (nn.Optimizer): Adam Optimizer\n","    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n","    @param batch_size (int): batch size\n","\n","    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n","    \"\"\"\n","    parser.model.train() # Places model in \"train\" mode, i.e. apply dropout layer\n","    n_minibatches = math.ceil(len(train_data) / batch_size)\n","    loss_meter = AverageMeter()\n","\n","    with tqdm(total=(n_minibatches)) as prog:\n","        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n","            optimizer.zero_grad()   # remove any baggage in the optimizer\n","            loss = 0. # store loss for this batch here\n","            train_x = torch.from_numpy(train_x).long()\n","            train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n","\n","            ### YOUR CODE HERE (~5-10 lines)\n","            ### TODO:\n","            ###      1) Run train_x forward through model to produce `logits`\n","            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n","            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n","            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n","            ###         are the predictions (y^ from the PDF).\n","            ###      3) Backprop losses\n","            ###      4) Take step with the optimizer\n","            ### Please see the following docs for support:\n","            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n","            logits=parser.model.forward(train_x)\n","\n","            loss=loss_func(logits,train_y)\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","\n","\n","\n","            ### END YOUR CODE\n","            prog.update(1)\n","            loss_meter.update(loss.item())\n","\n","    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n","\n","    print(\"Evaluating on dev set\",)\n","    parser.model.eval() # Places model in \"eval\" mode, i.e. don't apply dropout layer\n","    dev_UAS, _ = parser.parse(dev_data)\n","    print(\"here\")\n","    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n","    return dev_UAS\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"10VbHn6yuf-J","colab_type":"text"},"source":["#debug training mode\n"]},{"cell_type":"code","metadata":{"id":"Scog_XEnugMx","colab_type":"code","outputId":"6ec6ef7f-40d6-4df0-849e-09f088bebe1b","executionInfo":{"status":"ok","timestamp":1584406879106,"user_tz":-120,"elapsed":823671,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["debug=True\n","print(80 * \"=\")\n","print(\"INITIALIZING\")\n","print(80 * \"=\")\n","parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n","print(len(dev_data))\n","start = time.time()\n","model = ParserModel(embeddings)\n","parser.model = model\n","print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n","\n","print(80 * \"=\")\n","print(\"TRAINING\")\n","print(80 * \"=\")\n","output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n","output_path = output_dir + \"model.weights\"\n","\n","if not os.path.exists(output_dir):\n","  os.makedirs(output_dir)\n","\n","train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n","\n","\n","\n","if not debug:\n","    print(80 * \"=\")\n","    print(\"TESTING\")\n","    print(80 * \"=\")\n","    print(\"Restoring the best model weights found on the dev set\")\n","    parser.model.load_state_dict(torch.load(output_path))\n","    print(\"Final evaluation on test set\",)\n","    parser.model.eval()\n","    UAS, dependencies = parser.parse(test_data)\n","    print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n","    print(\"Done!\")\n","\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["================================================================================\n","INITIALIZING\n","================================================================================\n","Loading data...\n","took 2.06 seconds\n","Building parser...\n","took 0.03 seconds\n","Loading pretrained embeddings...\n","took 2.21 seconds\n","Vectorizing data...\n","took 0.05 seconds\n","Preprocessing training data...\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/48 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["took 1.23 seconds\n","500\n","took 0.00 seconds\n","\n","================================================================================\n","TRAINING\n","================================================================================\n","Epoch 1 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 48/48 [01:20<00:00,  1.26s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.7054593103627363\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["125250it [00:00, 9584509.97it/s]       \n","  0%|          | 0/48 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 52.58\n","New best dev UAS! Saving model.\n","\n","Epoch 2 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 48/48 [01:21<00:00,  1.26s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.35881108852724236\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["125250it [00:00, 9960686.68it/s]       \n","  0%|          | 0/48 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 58.89\n","New best dev UAS! Saving model.\n","\n","Epoch 3 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 48/48 [01:23<00:00,  1.33s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.2957000993192196\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["125250it [00:00, 12759869.23it/s]      \n","  0%|          | 0/48 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 62.18\n","New best dev UAS! Saving model.\n","\n","Epoch 4 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 48/48 [01:20<00:00,  1.25s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.25705418394257623\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["125250it [00:00, 9519381.29it/s]       \n","  0%|          | 0/48 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 65.21\n","New best dev UAS! Saving model.\n","\n","Epoch 5 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 48/48 [01:20<00:00,  1.25s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.22938586119562387\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["125250it [00:00, 11491809.42it/s]      \n","  0%|          | 0/48 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 67.77\n","New best dev UAS! Saving model.\n","\n","Epoch 6 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 48/48 [01:22<00:00,  1.28s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.20919993612915277\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["125250it [00:00, 11067406.32it/s]      \n","  0%|          | 0/48 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 68.50\n","New best dev UAS! Saving model.\n","\n","Epoch 7 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 48/48 [01:17<00:00,  1.23s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.19207833924641213\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["125250it [00:00, 10202295.04it/s]      \n","  0%|          | 0/48 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 68.64\n","New best dev UAS! Saving model.\n","\n","Epoch 8 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 48/48 [01:17<00:00,  1.21s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.17813265168418488\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["125250it [00:00, 9440858.59it/s]       \n","  0%|          | 0/48 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 71.59\n","New best dev UAS! Saving model.\n","\n","Epoch 9 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 48/48 [01:22<00:00,  1.28s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.16327177484830221\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["125250it [00:00, 14456550.15it/s]      \n","  0%|          | 0/48 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 71.63\n","New best dev UAS! Saving model.\n","\n","Epoch 10 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 48/48 [01:17<00:00,  1.21s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.15133895368004838\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["125250it [00:00, 10874057.17it/s]      "],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 72.32\n","New best dev UAS! Saving model.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"tOLZ4g7XBy7v","colab_type":"text"},"source":["#all dataset training mode"]},{"cell_type":"code","metadata":{"id":"sRtsfBOUBxEX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"48ccbed1-3d72-4eed-a18f-accf60b204d8","executionInfo":{"status":"ok","timestamp":1584414750777,"user_tz":-120,"elapsed":1129101,"user":{"displayName":"Zeyad Ezzat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCRc5TN1qVV5K_ZmIa4yalNbDnoZV6Z4KVlAgd_w=s64","userId":"07096185539114962531"}}},"source":["debug=False\n","print(80 * \"=\")\n","print(\"INITIALIZING\")\n","print(80 * \"=\")\n","parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n","print(len(dev_data))\n","start = time.time()\n","model = ParserModel(embeddings)\n","parser.model = model\n","print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n","\n","print(80 * \"=\")\n","print(\"TRAINING\")\n","print(80 * \"=\")\n","output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n","output_path = output_dir + \"model.weights\"\n","\n","if not os.path.exists(output_dir):\n","  os.makedirs(output_dir)\n","\n","train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n","\n","\n","\n","if not debug:\n","    print(80 * \"=\")\n","    print(\"TESTING\")\n","    print(80 * \"=\")\n","    print(\"Restoring the best model weights found on the dev set\")\n","    parser.model.load_state_dict(torch.load(output_path))\n","    print(\"Final evaluation on test set\",)\n","    parser.model.eval()\n","    UAS, dependencies = parser.parse(test_data)\n","    print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n","    print(\"Done!\")\n","\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["================================================================================\n","INITIALIZING\n","================================================================================\n","Loading data...\n","took 1.54 seconds\n","Building parser...\n","took 1.15 seconds\n","Loading pretrained embeddings...\n","took 2.11 seconds\n","Vectorizing data...\n","took 2.24 seconds\n","Preprocessing training data...\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/1848 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["took 35.64 seconds\n","1700\n","took 0.00 seconds\n","\n","================================================================================\n","TRAINING\n","================================================================================\n","Epoch 1 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1848/1848 [01:47<00:00, 17.13it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.19001993774013085\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["1445850it [00:00, 43223127.36it/s]      \n","  0%|          | 0/1848 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 83.78\n","New best dev UAS! Saving model.\n","\n","Epoch 2 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1848/1848 [01:45<00:00, 17.45it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.11678955810813806\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["1445850it [00:00, 44535679.74it/s]      \n","  0%|          | 0/1848 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 85.86\n","New best dev UAS! Saving model.\n","\n","Epoch 3 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1848/1848 [01:45<00:00, 17.54it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.10200648720968854\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["1445850it [00:00, 42092971.74it/s]      \n","  0%|          | 0/1848 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 86.98\n","New best dev UAS! Saving model.\n","\n","Epoch 4 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1848/1848 [01:45<00:00, 18.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.09314432441059387\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["1445850it [00:00, 45669639.64it/s]      \n","  0%|          | 0/1848 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 87.32\n","New best dev UAS! Saving model.\n","\n","Epoch 5 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1848/1848 [01:44<00:00, 19.40it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.08656773258008675\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["1445850it [00:00, 40027553.32it/s]      \n","  0%|          | 0/1848 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 87.95\n","New best dev UAS! Saving model.\n","\n","Epoch 6 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1848/1848 [01:44<00:00, 18.55it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.0810799139140585\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["1445850it [00:00, 42673823.88it/s]      \n","  0%|          | 0/1848 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 88.09\n","New best dev UAS! Saving model.\n","\n","Epoch 7 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1848/1848 [01:44<00:00, 17.15it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.07681888871668995\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["1445850it [00:00, 40938719.78it/s]      \n","  0%|          | 0/1848 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 88.31\n","New best dev UAS! Saving model.\n","\n","Epoch 8 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1848/1848 [01:44<00:00, 17.73it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.0729982198873188\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["1445850it [00:00, 43050988.46it/s]      \n","  0%|          | 0/1848 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 88.13\n","\n","Epoch 9 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1848/1848 [01:45<00:00, 17.55it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.06997848014867344\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["1445850it [00:00, 44840283.33it/s]      \n","  0%|          | 0/1848 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 88.47\n","New best dev UAS! Saving model.\n","\n","Epoch 10 out of 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1848/1848 [01:44<00:00, 18.92it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Average Train Loss: 0.06669180378422657\n","Evaluating on dev set\n"],"name":"stdout"},{"output_type":"stream","text":["1445850it [00:00, 45387308.41it/s]      \n"],"name":"stderr"},{"output_type":"stream","text":["here\n","- dev UAS: 88.48\n","New best dev UAS! Saving model.\n","\n","================================================================================\n","TESTING\n","================================================================================\n","Restoring the best model weights found on the dev set\n","Final evaluation on test set\n"],"name":"stdout"},{"output_type":"stream","text":["2919736it [00:00, 54433388.23it/s]      "],"name":"stderr"},{"output_type":"stream","text":["- test UAS: 89.28\n","Done!\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Wq286gHHuXil","colab_type":"code","colab":{}},"source":["if __name__ == \"__main__\":\n","    debug = args.debug\n","\n","    assert (torch.__version__.split(\".\") >= [\"1\", \"0\", \"0\"]), \"Please install torch version >= 1.0.0\"\n","\n","    print(80 * \"=\")\n","    print(\"INITIALIZING\")\n","    print(80 * \"=\")\n","    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n","\n","    start = time.time()\n","    model = ParserModel(embeddings)\n","    parser.model = model\n","    print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n","\n","    print(80 * \"=\")\n","    print(\"TRAINING\")\n","    print(80 * \"=\")\n","    output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n","    output_path = output_dir + \"model.weights\"\n","\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n","\n","    if not debug:\n","        print(80 * \"=\")\n","        print(\"TESTING\")\n","        print(80 * \"=\")\n","        print(\"Restoring the best model weights found on the dev set\")\n","        parser.model.load_state_dict(torch.load(output_path))\n","        print(\"Final evaluation on test set\",)\n","        parser.model.eval()\n","        UAS, dependencies = parser.parse(test_data)\n","        print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n","        print(\"Done!\")\n"],"execution_count":0,"outputs":[]}]}